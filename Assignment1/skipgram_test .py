# -*- coding: utf-8 -*-
"""skipgram_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/102K1GUvChPYZkGCJPMavzyoxQtRXnL53
"""

from Skipgram.py import *

parser.add_argument('--checkpoint_dir', help='Save checkpoints to this directory', required=True, type=str)
checkpoint_dir=args.checkpoint_dir

import torch
# Load the saved state dict from the checkpoint
checkpoint = torch.load("/content/drive/MyDrive/CS772 data/checkpoint_step000000000.pth")
#checkpoint = torch.load(checkpoint_dir)
model.load_state_dict(checkpoint['state_dict'])

# If you saved the optimizer state as well, you can also reload it
#optimizer = YourOptimizerClass(model.parameters(), lr=0.01)
#optimizer.load_state_dict(checkpoint['optimizer_dict'])
for name, param in model.named_parameters():
    print(name, param.data)

word_embeddings=model.embeddings.weight

prompt = "Enter the word analogy: "
input_text = input(prompt)

input_text=input_text.split(":")
input_text.remove('')

word1=input_text[0]
word2=input_text[1]
word3=input_text[2]

from sklearn.metrics.pairwise import cosine_distances
  if word1 and word2 and word3 in vocab:
    word_vector_1=word_embeddings[word2index[word1]]
    word_vector_2=word_embeddings[word2index[word2]]
    word_vector_3=word_embeddings[word2index[word3]]
  else:
    print("words not in vocabulary")
  predicted_word_embedding=word_vector_1-word_vector_2+word_vector_3

top_indexes=[]
for i in range(0,len(word_embeddings)):
  distance = cosine_distances(word_embeddings[i].detach().numpy().reshape(1, -1), predicted_word_embedding.detach().numpy().reshape(1, -1))
  if distance[0][0] <= 0.4:
    top_indexes.append(i)

print("predicted words: ")
for index in top_indexes:
  print(index2word[index])