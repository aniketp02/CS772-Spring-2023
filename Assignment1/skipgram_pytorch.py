# -*- coding: utf-8 -*-
"""Skipgram_pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15PDkoJpTy611D59y53KebSFR2-crRCTF
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1)

import re

data_file = open("/content/drive/MyDrive/CS772 data/final_data (1).txt", 'r')
raw_text = data_file.readline()

raw_text = raw_text

raw_text

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

# cleaning the dataset
sentences = re.sub('[^A-Za-z0-9]+', ' ', raw_text) # remove special characters
pattern = r'[0-9]'
sentences= re.sub(pattern, '', sentences)
sentences = re.sub(r'(?:^| )\w(?:$| )', ' ', sentences).strip() # remove 1 letter words
sentences = sentences.lower().split() # lower all characters
print("Preprocessed the dataset")

sentences

vocab = set(sentences)

vocab=list(vocab)

vocab = set(sentences)
vocab_size = len(vocab)

vocab

vocab_size

import random

# Define the vocabulary and the corresponding index
vocab = sentences
word2index = {word: i for i, word in enumerate(vocab)}
index2word = {i: word for i, word in enumerate(vocab)}

# Generate the training data
data = []
window_size = 2
for i, word in enumerate(vocab):
    for j in range(i-window_size, i+window_size+1):
        if i != j and j >= 0 and j < len(vocab):
            data.append((word2index[word], word2index[vocab[j]]))

# Shuffle the data
#random.shuffle(data)

# Define the number of epochs
num_epochs = 5

# Define the size of the vocabulary
vocab_size = len(vocab)

word2index

index2word[data[1][0]]

checkpoint_dir = "/content/drive/MyDrive/CS772 data"

from os.path import join
def save_checkpoint(model, optimizer, checkpoint_dir, epoch):
    checkpoint_path = join(checkpoint_dir, "checkpoint_step{:09d}.pth".format(epoch))
    torch.save({
        "state_dict": model.state_dict(),
        "optimizer": optimizer.state_dict(),
    }, checkpoint_path)
    print("Saved checkpoint:", checkpoint_path)

import torch
import torch.nn as nn
import torch.optim as optim

class SkipGramModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(SkipGramModel, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.linear = nn.Linear(embedding_dim, vocab_size)
        
    def forward(self, inputs):
        embeds = self.embeddings(inputs).view((1, -1))
        out = self.linear(embeds)
        log_probs = nn.LogSoftmax(dim=1)(out)
        return log_probs

def train(data, model, optimizer, criterion, num_epochs):
    for epoch in range(num_epochs):
        total_loss = 0
        for context, target in data:
            context_var = torch.tensor([context], dtype=torch.long)
            target_var = torch.tensor([target], dtype=torch.long)
            model.zero_grad()
            log_probs = model(context_var)
            loss = criterion(log_probs, target_var)
            loss.backward()
            optimizer.step()
        
            total_loss += loss.item()
        print("Epoch %d: Loss = %.4f" % (epoch+1, total_loss))
        if epoch % 3 == 0:
          save_checkpoint(model, optimizer, checkpoint_dir, epoch)
        
# Define the model and optimizer
embedding_dim = 10
model = SkipGramModel(vocab_size, embedding_dim)
optimizer = optim.SGD(model.parameters(), lr=0.001)
criterion = nn.NLLLoss()

# Train the model
train(data, model, optimizer, criterion, num_epochs=5)
word_vectors = model.embeddings.weight.data

word_vectors

model.eval()

word2index['been']

word_vectors[word2index['been']]

"""**VALIDATION**"""

import torch
# Load the saved state dict from the checkpoint
checkpoint = torch.load("/content/drive/MyDrive/CS772 data/checkpoint_step000000000.pth")
model.load_state_dict(checkpoint['state_dict'])

# If you saved the optimizer state as well, you can also reload it
#optimizer = YourOptimizerClass(model.parameters(), lr=0.01)
#optimizer.load_state_dict(checkpoint['optimizer_dict'])
for name, param in model.named_parameters():
    print(name, param.data)

word_embeddings=model.embeddings.weight

word_embeddings[1]

import torch

# Load the model from the checkpoint
checkpoint = torch.load("model.pt")
model = checkpoint['model']
model.load_state_dict(checkpoint['state_dict'])

# Access the values of the weights
for name, param in model.named_parameters():
    print(name, param.data)

validation_text=open("/content/drive/MyDrive/CS772 data/Validation.txt", 'r')
validation_text = validation_text.read()

validation_text=validation_text.split('\n')

validation_text[0]

import numpy as np
def cosine_distance(u, v):
    dot = np.dot(u, v)
    norm_u = np.linalg.norm(u)
    norm_v = np.linalg.norm(v)
    cosine_similarity = dot / (norm_u * norm_v)
    distance = 1 - cosine_similarity
    return distance

from sklearn.metrics.pairwise import cosine_distances
words=[]
for lines in validation_text[0]:
  words=[lines.split(" ")]
  if words in vocab:
    word_vector_1=word_embeddings[word2index[words[0]]]
    word_vector_2=word_embeddings[word2index[words[1]]]
    word_vector_3=word_embeddings[word2index[words[2]]]
    word_vector_4=word_embeddings[word2index[words[3]]]
    predicted_word_embedding=word_vector_1-word_vector_2+word_vector_3
    dist_exp_true = cosine_distances(word_vector_4, predicted_word_embedding)
  else :
    print("words not in dictionary")

words=[lines.split(" ")]
  word_vector_1=word_embeddings[word2index["family"]]
  word_vector_2=word_embeddings[word2index["less"]]
  word_vector_3=word_embeddings[word2index['friend']]
  word_vector_4=word_embeddings[word2index['fond']]
  print(word_vector_4)

  predicted_word_embedding=word_vector_1-word_vector_2+word_vector_3
  print(predicted_word_embedding)
  dist_exp_true = cosine_distance(word_vector_4.detach().numpy(), predicted_word_embedding.detach().numpy())

cos_sim = nn.CosineSimilarity(dim=1, eps=1e-6)
similarities = cos_sim(word_vector_4, predicted_word_embedding)
    
_, closest_word_index = torch.max(similarities, 0)
closest_word = index2word[closest_word_index.item()]

print(dist_exp_true)