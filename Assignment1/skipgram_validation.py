# -*- coding: utf-8 -*-
"""Skipgram_validation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1feDD_j-flE1sV8OeUzkY-LWT1417ak1n
"""

from Skipgram.py import *

parser.add_argument("--val_data", help="Root path (.txt) of the validation dataset", required=True, type=str)
parser.add_argument('--checkpoint_dir', help='Save checkpoints to this directory', required=True, type=str)
args = parser.parse_args()

checkpoint_dir=args.checkpoint_dir
val_data=args.val_data

import torch
# Load the saved state dict from the checkpoint
#checkpoint = torch.load("/content/drive/MyDrive/CS772 data/checkpoint_step000000000.pth")
checkpoint = torch.load(checkpoint_dir)
model.load_state_dict(checkpoint['state_dict'])

# If you saved the optimizer state as well, you can also reload it
#optimizer = YourOptimizerClass(model.parameters(), lr=0.01)
#optimizer.load_state_dict(checkpoint['optimizer_dict'])
for name, param in model.named_parameters():
    print(name, param.data)

word_embeddings=model.embeddings.weight

#validation_text=open("/content/drive/MyDrive/CS772 data/Validation.txt", 'r')
validation_text = open(val_data,"r")
validation_text = validation_text.read()

validation_text=validation_text.split('\n')

from sklearn.metrics.pairwise import cosine_distances
words=[]
for lines in validation_text:
  words=[lines.split(" ")]
  if words[0] and words[1] and words[2] and words[3] in vocab:
    word_vector_1=word_embeddings[word2index[words[0]]]
    word_vector_2=word_embeddings[word2index[words[1]]]
    word_vector_3=word_embeddings[word2index[words[2]]]
    word_vector_4=word_embeddings[word2index[words[3]]]
    predicted_word_embedding=word_vector_1-word_vector_2+word_vector_3
    dist_exp_true = cosine_distances(word_vector_4, predicted_word_embedding)

    print(words[0],":",words[1],"::",words[2],":"," possible words are as follows:" )

    top_indexes=[]
    for i in range(0,len(word_embeddings)):
      distance = cosine_distances(word_embeddings[i].detach().numpy().reshape(1, -1), predicted_word_embedding.detach().numpy().reshape(1, -1))
      if distance[0][0] <= 0.2:
        top_indexes.append(i)
    for index in top_indexes:
      print(index2word[index])
  else :
    print("words not in dictionary")