# -*- coding: utf-8 -*-
"""nlp_ass_pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hQ0d9YZdTqp9FYYDm4MaPYE3kj7pKtsS
"""

import nltk
from nltk.corpus import gutenberg
nltk.download('gutenberg')
nltk.download('punkt')
import regex as re
from nltk.corpus import stopwords
nltk.download('stopwords')
import string

import json
with open(r'/content/drive/MyDrive/final_sentences (1).json') as f:
  final_sentences = json.load(f)

final_sentences[1]

def preprocessing(corpus):
    stop_words = set(stopwords.words('english'))   
    training_data = []
    for sentence in corpus:
        x = []
        for word in sentence:
            word = word.strip(string.punctuation)
            word = word.strip()
            word = re.sub(r"(http[s]?\://\S+)|([\[\(].*[\)\]])|([#@]\S+)|\n|([0-9])|(\,+)|(\'+)|(\"+)|([^\w\s])", '', word.lower())
           
            if word.lower() not in stop_words and word != '':
                word = word.lower()
                x.append(word)
        training_data.append(x)
    print(training_data)
    return training_data



import torch

def prepare_data_for_training(sentences):
    data = {}
    for sentence in sentences:
        for word in sentence:
            if word not in data:
                data[word] = 1
            else:
                data[word] += 1
    V = len(data)
    data = sorted(list(data.keys()))
    vocab = {}
    for i in range(len(data)):
        vocab[data[i]] = i
      
    N = sum([len(sentence) for sentence in sentences])
    X_train = torch.zeros((N, V), dtype=torch.float)
    y_train = torch.zeros((N, V), dtype=torch.float)
    k = 0
    for sentence in sentences:
        for i in range(len(sentence)):
            X_train[k, vocab[sentence[i]]] = 1
            for j in range(i-2,i+2):
                if i!=j and j>=0 and j<len(sentence):
                    y_train[k, vocab[sentence[j]]] += 1
            k += 1
    
    print(y_train.shape)
    print(y_train[0,].shape)
    
    return X_train,y_train,V,data

training = preprocessing(final_sentences[0:10000])

X_train,Y_train,V,data = prepare_data_for_training(training)

from google.colab import drive
drive.mount('/content/drive')

vocab_size = V
embedding_dims = 5
if torch.cuda.is_available():
    device = torch.device('cuda')
    
else:
    device = torch.device('cpu')
    

initrange = 0.5 / embedding_dims

num_epochs = 10
learning_rate = 0.001
lr_decay = 0.99
loss_hist = []

import torch
torch.manual_seed(10)
from torch.autograd import Variable
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np
from sklearn import decomposition
from pathlib import Path
import warnings
warnings.filterwarnings("ignore")
import seaborn as sns
from matplotlib import pyplot as plt
plt.rcParams['figure.figsize'] = (10,8)
import nltk
import torch.nn.functional as F
#Import stopwords
from nltk.corpus import stopwords
W1 = Variable(torch.randn(vocab_size, embedding_dims, device=device).uniform_(-initrange, initrange).float()).to(device) # shape V*H
W2 = Variable(torch.randn(embedding_dims, vocab_size, device=device).uniform_(-initrange, initrange).float()).to(device)

X_train

class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return json.JSONEncoder.default(self, obj)
for epo in range(num_epochs):
      
        # one-hot encode input tensor
         #shape N*V
    X_train = X_train.to(device)
    Y_train = Y_train.to(device)
    
    
    def forward(X):
        
        
        h = torch.matmul(W1.T,X).reshape(embedding_dims, 1)
        u = torch.matmul(W2.T,h)
        y = F.softmax(u, dim=0)
        return y,h,u

    def backpropagate(X, t,h,u,p):
        # Read the data from the file and convert it to a list
        

          e = t - torch.tensor(p).reshape(len(t),1)

            
          dLdW1 = torch.matmul(e,h.T)
        
          dLdW2 = (torch.matmul(X.reshape(vocab_size,1), torch.matmul(W1.T, e).T)).T
          W1.data = W1.data - learning_rate * dLdW1
          W2.data = W2.data - learning_rate * dLdW2
    
    
    
    #compute loss
    
    
    # bakpropagation step
    
    
    
    
        
    for j in range(len(X_train)):
        y_result,h,u = forward(X_train[j])
        p = Y_train[j]
        backpropagate(X_train[j], y_result,h,u,p)
        C = torch.tensor(0).to(device)
        loss = torch.zeros((1,1)).to(device)
        #for m in range(len(Y_train[j])):
            #if Y_train[j][m]:
                #loss += -1 * u[m][0]
                #C += 1
        u = u.clamp(min=1e-6)

        loss += torch.log(torch.sum(u))
    
    
    # Update weights using gradient descent. For this step we just want to mutate
    # the values of w1 and w2 in-place; we don't want to build up a computational
    # graph for the update steps, so we use the torch.no_grad() context manager
    # to prevent PyTorch from building a computational graph for the updates
    
    '''W1.grad.data.zero_()
    W1.grad.data.zero_()'''
    if epo%10 == 0:
        learning_rate *= lr_decay
  
    if epo%2 == 0:
        print(f'Epoch {epo}, loss = {loss}')
    if epo == num_epochs - 1:
        output = {}
        for i in range(vocab_size):
            word = data[i]
            n = [0 for j in range(vocab_size)]
            n[i] = 1
            n = torch.tensor(n,dtype=torch.float)
            
            W1 = W1.cpu()
            prediction = torch.matmul(W1.T,n.T).reshape(1,embedding_dims)
            prediction_numpy = prediction.numpy()
            output[word] = prediction_numpy
        with open(r'/content/output.json','w') as f:
            json.dump(output, f, cls=NumpyEncoder)

def accuracy():